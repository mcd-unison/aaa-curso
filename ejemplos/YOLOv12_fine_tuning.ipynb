{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png)\n",
        "\n",
        "# YOLOv12 Finetuning\n",
        "\n",
        "## Aprendizaje Automático Aplicado\n",
        "\n",
        "### Maestría en Ciencia de Datos\n",
        "\n",
        "#### **Julio Waissman**, 2025\n",
        "\n",
        "[**Abrir en google Colab**](https://colab.research.google.com/github/mcd-unison/aaa-curso/blob/main/ejemplos/YOLOv12_fine_tuning.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## YOLO (You Only Look Once)\n",
        "\n",
        "El modelo YOLO (You Only Look Once) para la detección de objetos funciona de una manera bastante ingeniosa. En lugar de analizar múltiples regiones de una imagen como otros detectores, YOLO divide la imagen en una cuadrícula. Cada celda de esta cuadrícula es responsable de predecir objetos cuyos centros caen dentro de ella. Para cada celda, el modelo predice simultáneamente múltiples cajas delimitadoras (rectángulos que encierran los objetos), la confianza de que cada caja contenga un objeto, y las probabilidades de clase de ese objeto (por ejemplo, perro, coche, persona).\n",
        "\n",
        "Esencialmente, YOLO realiza una única pasada a través de la red neuronal para hacer todas estas predicciones a la vez, lo que le confiere su gran velocidad. Las cajas delimitadoras predichas se filtran según su puntuación de confianza y se aplican técnicas como la supresión no máxima para eliminar las detecciones redundantes del mismo objeto. Este enfoque de \"mirar una sola vez\" permite una detección de objetos en tiempo real muy eficiente.\n",
        "\n",
        "El artículo original que introduce la arquitectura YOLO es [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/pdf/1506.02640). De la versión original se han derivado varios modelos con mejoras, siendo el último (a la fecha de este ejemplo) la versión 12, de febrero de 2025.\n",
        "\n",
        "YOLO12 introduce una arquitectura centrada en la atención que se aleja de los enfoques tradicionales basados en CNN utilizados en modelos YOLO anteriores, pero conserva la velocidad de inferencia en tiempo real esencial para muchas aplicaciones. Este modelo consigue la máxima precisión en la detección de objetos gracias a novedosas innovaciones metodológicas en los mecanismos de atención y en la arquitectura general de la red, al tiempo que mantiene el rendimiento en tiempo real. El reporte técnico donde se presenta es [YOLOv12: Attention-Centric Real-Time Object Detectors](https://arxiv.org/pdf/2502.12524).\n",
        "\n",
        "En general los modelos YOLO son abiertos y libres de uso con licencias tipo [GNU Affero General Public License v3.0](https://github.com/sunsmarterjie/yolov12/blob/main/LICENSE). Los modelos se distributen a través de la empresa [Ultralytics](https://www.ultralytics.com/es/yolo) y para realizar el *fine tuning* lo más complicado es poner el conjunto de datos de interés en el formato `COCODataset`.\n",
        "\n",
        "Veamos como hacer esto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Instalando y cargando modulos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vamos ainstalar algunas liberias que no vienen en colab por default\n",
        "\n",
        "# Para usar los modelos YOLO\n",
        "!pip install ultralytics --quiet\n",
        "\n",
        "# Para recuperar los datos que vamos a usar de ejemplo\n",
        "!pip install kagglehub --quiet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHoZdm7KuoTf",
        "outputId": "545dbba3-6826-4b8c-a947-3729308dd0ad"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from ultralytics import YOLO\n",
        "import kagglehub\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import xml.etree.ElementTree as ET\n",
        "import shutil\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Descargando datos de Kaggle\n",
        "\n",
        "Para este ejemplo vamos a usar un conjunto de datos que se usa comunmente para detección de objetos, el [HRSC2016-MS Dataset](https://www.kaggle.com/datasets/weiming97/hrsc2016-ms-dataset) que son datos a multiples resoluciones donde se detectan navíos en imagenes de Google Earth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqxlmdV9u443"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/HRSC2016-MS/\"\n",
        "\n",
        "path = kagglehub.dataset_download(\"weiming97/hrsc2016-ms-dataset\")\n",
        "print(\"Los archivos se descargaron en:\", path)\n",
        "print(\"Pero los vamos a pasar a: \", dataset_path)\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    os.makedirs(dataset_path)\n",
        "    os.system(f\"cp -r {path}/* {dataset_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Creando un directorio con datos al estilo COCODataset\n",
        "\n",
        "La parte complicada es como leer el archivo en formato XML con la información de los objetos, tal como está codificado en al base de datos y convertirlos al formato estandar que utiliza YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def xml2txt(xml_file_path, txt_file, w, h):\n",
        "    tree = ET.parse(xml_file_path)\n",
        "    root = tree.getroot()\n",
        "    objs = root.findall('object')\n",
        "\n",
        "    if not objs:\n",
        "        print(f\"Warning: No objects found in {xml_file_path}. Creating an empty annotation file.\")\n",
        "        with open(txt_file, 'w') as f:\n",
        "            f.write(\"\\n\")  # Empty file for compatibility\n",
        "        return\n",
        "\n",
        "    with open(txt_file, 'w') as f:\n",
        "        for obj in objs:\n",
        "            try:\n",
        "                xmin = int(obj.find('bndbox/xmin').text)\n",
        "                ymin = int(obj.find('bndbox/ymin').text)\n",
        "                xmax = int(obj.find('bndbox/xmax').text)\n",
        "                ymax = int(obj.find('bndbox/ymax').text)\n",
        "\n",
        "                # Convert to YOLO format (normalized values)\n",
        "                x_center = (xmin + xmax) / (2.0 * w)\n",
        "                y_center = (ymin + ymax) / (2.0 * h)\n",
        "                box_w = (xmax - xmin) / w\n",
        "                box_h = (ymax - ymin) / h\n",
        "\n",
        "                # Save in YOLO format\n",
        "                f.write(f\"0 {x_center:.6f} {y_center:.6f} {box_w:.6f} {box_h:.6f}\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {xml_file_path}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora creamos un sistema de archivos como el que necesitamos y lo guardamas en `yolo_dir`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cwd = os.getcwd()\n",
        "yolo_dir = os.path.join(cwd, 'HRSC-YOLO')\n",
        "os.mkdir(yolo_dir)\n",
        "\n",
        "for i in ['train', 'val', 'test']:\n",
        "    folder = os.path.join(yolo_dir, i)\n",
        "    os.mkdir(folder)\n",
        "    for j in ['images', 'labels']:\n",
        "        subfolder = os.path.join(folder, j)\n",
        "        os.mkdir(subfolder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "y ahora leemos los archivos en el formato que vienen en el ejemplo y los guardamos como se solicita en Yolo, vamos a hacer una función para generalizar este proceso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convierte_imagenes_para_yolo(tipo, dataset_path, yolo_dir):\n",
        "    if tipo not in ['train', 'val', 'test']:\n",
        "        raise ValueError(\"tipo debe ser 'train', 'val' or 'test'\")\n",
        "    \n",
        "    with open(os.path.join(dataset_path, f'ImageSets/{tipo}.txt'), 'r') as f:\n",
        "        lista_tipo = f.read().splitlines()\n",
        "    \n",
        "    for file_name in tqdm(lista_tipo): \n",
        "        bmp_path = os.path.join(dataset_path, f'AllImages/{file_name}.bmp') \n",
        "        xml_path = os.path.join(dataset_path, f'Annotations/{file_name}.xml') \n",
        "        txt_path = os.path.join(yolo_dir, f'{tipo}/labels/{file_name}.txt') \n",
        "        png_path = os.path.join(yolo_dir, f'{tipo}/images/{file_name}.png')\n",
        "\n",
        "        img = Image.open(bmp_path)\n",
        "        w, h = img.size\n",
        "        xml2txt(xml_path, txt_path, w, h)\n",
        "        img.save(png_path, format='PNG')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y ahora generamos las imágenes al estilo YOLO:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "convierte_imagenes_para_yolo('train', \"/content/HRSC2016-MS\", yolo_dir)\n",
        "convierte_imagenes_para_yolo('val', \"/content/HRSC2016-MS\", yolo_dir)\n",
        "convierte_imagenes_para_yolo('test', \"/content/HRSC2016-MS\", yolo_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Volver a acomodar las imágenes y crear el archivo yaml\n",
        "\n",
        "De nuevo vamos a poner todas las imágenes juntas y todas las etiquetas juntas, sin que nos importe cuales son de entrenamiento, prueba o validación. Y vamos a rehacer conjuntos nuevos de entrenamiento, prueba y validación a partir de todas las imagenes juntas.\n",
        "\n",
        "Empezamos por ponerlas todas juntas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMHSRa9l7cR6",
        "outputId": "4c1c8e21-abfc-4360-a5a2-bda0c9f55892"
      },
      "outputs": [],
      "source": [
        "source_dir = \"/content/HRSC-YOLO/\"\n",
        "dest_dir = \"/content/experimentals/\"\n",
        "\n",
        "os.makedirs('experimentals', exist_ok=True)\n",
        "os.makedirs(os.path.join(dest_dir, \"images\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(dest_dir, \"labels\"), exist_ok=True)\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    image_source = os.path.join(source_dir, split, \"images\")\n",
        "    label_source = os.path.join(source_dir, split, \"labels\")\n",
        "\n",
        "    for file in os.listdir(image_source):\n",
        "        src_path = os.path.join(image_source, file)\n",
        "        dest_path = os.path.join(dest_dir, \"images\", file)\n",
        "        shutil.copy2(src_path, dest_path)\n",
        "\n",
        "    for file in os.listdir(label_source):\n",
        "        src_path = os.path.join(label_source, file)\n",
        "        dest_path = os.path.join(dest_dir, \"labels\", file)\n",
        "        shutil.copy2(src_path, dest_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y aqui, de manera bastante complicada, reconstruyen un nuevo conjunto de entrenamiento, prueba y validación, usando 610 imagenes de entrenamiento, 460 de validación y 610 de prueba:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bk1YsV2UvXYA",
        "outputId": "c66c21e4-2257-4f9d-a4af-70e9fd924984"
      },
      "outputs": [],
      "source": [
        "# Define paths\n",
        "base_dir = \"/content/experimentals\"\n",
        "images_dir = os.path.join(base_dir, \"images\")\n",
        "labels_dir = os.path.join(base_dir, \"labels\")\n",
        "\n",
        "# Output directories\n",
        "output_dirs = {\n",
        "    \"train\": \"train\",\n",
        "    \"val\": \"val\",\n",
        "    \"test\": \"test\"\n",
        "}\n",
        "\n",
        "# Create train, val, and test directories with images and labels subfolders\n",
        "for split in output_dirs.values():\n",
        "    os.makedirs(os.path.join(split, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(split, \"labels\"), exist_ok=True)\n",
        "\n",
        "# Get all image filenames\n",
        "image_files = [f for f in os.listdir(images_dir) if f.endswith(\".png\")]\n",
        "random.shuffle(image_files)  # Shuffle dataset\n",
        "\n",
        "# Define split sizes\n",
        "train_size, val_size, test_size = 610, 460, 610\n",
        "\n",
        "# Split dataset\n",
        "train_images = image_files[:train_size]\n",
        "val_images = image_files[train_size:train_size + val_size]\n",
        "test_images = image_files[train_size + val_size:train_size + val_size + test_size]\n",
        "\n",
        "# Function to copy files\n",
        "def copy_files(image_list, split):\n",
        "    for img_name in image_list:\n",
        "        label_name = img_name.replace(\".png\", \".txt\")  # Corresponding label\n",
        "        shutil.copy(os.path.join(images_dir, img_name), os.path.join(split, \"images\", img_name))\n",
        "        shutil.copy(os.path.join(labels_dir, label_name), os.path.join(split, \"labels\", label_name))\n",
        "\n",
        "# Copy files to respective folders\n",
        "copy_files(train_images, \"train\")\n",
        "copy_files(val_images, \"val\")\n",
        "copy_files(test_images, \"test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y ahora establecemos el archivo YAML para el entrenamiento, que podría haber sido con el conjunto tal como lo tenñiamos originalmente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDFiBw6svl7x"
      },
      "outputs": [],
      "source": [
        "yolo_dir2 = \"/content\"\n",
        "data_yaml = os.path.join(yolo_dir2, \"data.yaml\")\n",
        "\n",
        "yaml_content = f\"\"\"\n",
        "train: {os.path.join(yolo_dir2, 'train/images')}\n",
        "val: {os.path.join(yolo_dir2, 'val/images')}\n",
        "test: {os.path.join(yolo_dir2, 'test/images')}\n",
        "\n",
        "nc: 1\n",
        "names: ['ship']\n",
        "\"\"\"\n",
        "\n",
        "with open(data_yaml, \"w\") as f:\n",
        "    f.write(yaml_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Cargando el modelo y entrenandolo\n",
        "\n",
        "Esto es la parte menos divertida con todo lo que ya viene en la librería de Ultralitics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Descarga YOLOv12-large model de https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo12l.pt and se sube a colab\n",
        "\n",
        "!wget https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo12l.pt -O \\\n",
        "    /content/yolo12l.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y ahora solo carga y entrena el modelo con las herramientas que ya existen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7K0DinVv23K"
      },
      "outputs": [],
      "source": [
        "model = YOLO(\"/content/yolo12l.pt\")\n",
        "\n",
        "model.train(\n",
        "    data=data_yaml, \n",
        "    epochs=100, \n",
        "    batch=8, \n",
        "    imgsz=640, \n",
        "    device='cuda', \n",
        "    workers=4, \n",
        "    save=True, \n",
        "    save_period=10\n",
        ")\n",
        "model.val()\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y la validación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mejor = \"/content/runs/detect/train/weights/best.pt\"\n",
        "modelo = YOLO(mejor)\n",
        "metrics = model.val(data=data_yaml)\n",
        "print(metrics.box.map)\n",
        "print(metrics.box.map50)\n",
        "print(metrics.box.map75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Haciendo inferencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mdxy-SgTwNlD"
      },
      "outputs": [],
      "source": [
        "def infer_yolov12(image_path, model_weights):\n",
        "    \"\"\"\n",
        "    path to the saved model weights \n",
        "    can be found at runs/detect/trainx/weights/best.pt\n",
        "    \n",
        "    \"\"\"\n",
        "    model = YOLO(model_weights)\n",
        "    results = model(image_path)\n",
        "    \n",
        "    for result in results:\n",
        "        img = cv2.imread(image_path)\n",
        "        for box in result.boxes:\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            conf = box.conf[0].item()\n",
        "            cls = int(box.cls[0])\n",
        "            label = f\"{model.names[cls]} {conf:.2f}\"\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xq6OR7vwZ2l"
      },
      "outputs": [],
      "source": [
        "test_image = \"/content/HRSC-YOLO/train/images/100001058.png\"\n",
        "infer_yolov12(test_image)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
