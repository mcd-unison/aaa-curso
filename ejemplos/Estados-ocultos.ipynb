{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<p><img src=\"https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png\" width=\"150\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "<h1>Redes Neuronales Recurrentes (RNN) con numpy</h1>\n",
    "\n",
    "<h3>Curso Aprendizaje Automático Aplicado</h3>\n",
    "\n",
    "\n",
    "<p> Julio Waissman Vilanova </p>\n",
    "\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mcd-unison/pln/blob/main/labs/RNN/Estados-ocultos.ipynb\"><img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\"  width=\"30\" /> Ejecuta en Colab</a>\n",
    "\n",
    "<p>\n",
    "Tomado parcialmente y adaptado de libretas de la <i>Especialización en procesamiento de lenguaje natural</i> de <i>Deeplearning.ai</i>, disponible en <i>Coursera</i>.\n",
    "</p>\n",
    "\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN vainilla\n",
    "\n",
    "Para una RNN simple como se muestra en la figura:\n",
    "\n",
    "![vanilla rnn](vanilla_rnn.PNG)\n",
    "\n",
    "\n",
    "La activación de los estados ocultos están dados por:      \n",
    "\n",
    "$h^{<t>}=g(W_{hh}h^{<t-1>} + W_{hx}x^{<t>} + b_h)$                                        \n",
    "\n",
    "\n",
    "este ejemplo lo vams a hacer usando exclusivamente `numpy` para entender el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos entonces a desarrollar la función de alimentación a adelante de una RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Calcula la función logística\n",
    "    \n",
    "    ## INICIA CODIGO\n",
    "    return None\n",
    "    ## ACABA CODIGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_V_RNN(inputs, weights): \n",
    "    # Forward propagation para una RNN vanilla \n",
    "    x_t, h_t_prev = inputs\n",
    "\n",
    "    # weights.\n",
    "    w_hh, w_xh, b_h = weights\n",
    "\n",
    "    ### INICIA CODIGO ###\n",
    "    # Nuevo estado oculto\n",
    "    \n",
    "    # Operación lineal\n",
    "    z_t = None\n",
    "    \n",
    "    # Activación\n",
    "    h_t = None\n",
    "\n",
    "    ### ACABA CODIGO ###\n",
    "\n",
    "    return h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar como funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "nh = 2   # Dimensión del vector de variables ocultas\n",
    "nx = 3   # Dimensión del vector de entrada\n",
    "\n",
    "w_hh = np.full((nh, nh), 1.)  # 3x2 llenado con puros 1s\n",
    "w_hx = np.full((nh, nx), 9.)  # 3x3 llenado con puros 9s\n",
    "h_t_prev = np.full((nh, 1), 1.)  # 2x1 llenado con puros 1s\n",
    "x_t = np.full((nx, 1), 9.)       # 3x1 llenado con puros 9s\n",
    "b_h = np.zeros((nh, 1))\n",
    "\n",
    "# Si prefieres valores aleatorios, descomenta lo siguiente:\n",
    "\n",
    "# w_hh = np.random.standard_normal((nh,nh))\n",
    "# w_hx = np.random.standard_normal((nh,nx))\n",
    "# h_t_prev = np.random.standard_normal((nh,1))\n",
    "# x_t = np.random.standard_normal((nx,1))\n",
    "\n",
    "# Aplicando un solo paso\n",
    "h_t = forward_V_RNN([x_t, h_t_prev], [w_hh, w_hx, b_h])\n",
    "\n",
    "print(\"\\nValor h_t:\")\n",
    "print(h_t, \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN tipo LSTM\n",
    "\n",
    "Una LST es un modelo como el que se muestra en la figura, con todo y sus ecuaciones\n",
    "\n",
    "![](LSTM.png)\n",
    "\n",
    "![](LSTMeq.png)\n",
    "\n",
    "Como podemos ver tenemos 3 vectores de entrada a la celda:\n",
    "\n",
    "- $h^{<t-1>}$ el vector de variables ocultas provenientes de un paso anterior,\n",
    "- $C^{<t-1>}$ el vector de valores de celda (memoria de largo plazo) provenientes de un paso anterior,\n",
    "- $x^{<t>}$ el vector de variables de entrada. Idealmente debería estar normalizado entre -1 y 1 cada uno de los valores de entrada.\n",
    "\n",
    "Como podemos ver tenemos varias operaciones:\n",
    "\n",
    "- Una compuerta de olvido $f$ que depende de $h^{<t-1>}$ y $x^{<t>}$ cuya salida es un vector del tamaño de las variables ocultas con valores entre 0 y 1 con la importancia que debe tener el valor de celda anterior (memoria de largo plazo)\n",
    "\n",
    "- Una compuerta de entrada $i$ que depende de $h^{<t-1>}$ y $x^{<t>}$ cuya salida es un vector del tamaño de las variables ocultas con valores entre 0 y 1 con la importancia que debe tener la activación de la celda actual (memoria de corto plazo)\n",
    "\n",
    "- Una compuerta de salida $i$ que depende de $h^{<t-1>}$ y $x^{<t>}$ cuya salida es un vector del tamaño de las variables ocultas con valores entre 0 y 1 con la importancia que debe tener el valor de celda actual en el valor de la de la variable oculta correspondiente.\n",
    "\n",
    "- El calculo de la activación actual, que depende de $h^{<t-1>}$ y $x^{<t>}$, el cual se hace con una tangente hiperbólica, para mantener los valores entre -1 y 1.\n",
    "\n",
    "\n",
    "Hagamos entonces una celda LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_LSTM(inputs, weights): \n",
    "    # Forward propagation para una RNN tipo LSTM \n",
    "    x_t, h_t_prev, C_t_prev = inputs\n",
    "\n",
    "    # weights.\n",
    "    Ui, Wi, Uf, Wf, Uo, Wo, U, W = weights\n",
    "\n",
    "    ### INICIA CODIGO ###\n",
    "    # Nuevo estado oculto y valor de celda\n",
    "    \n",
    "    # Compuerta de entrada\n",
    "    i = None\n",
    "    \n",
    "    # Compuerta de olvido\n",
    "    f = None\n",
    " \n",
    "    # Compuerta de salida\n",
    "    o = None\n",
    "    \n",
    "    # Valor de celda de memoria de corto plazo\n",
    "    C_t_short = None\n",
    "    \n",
    "    # Valor de celda de memoria de corto y largo plazo\n",
    "    C_t = None\n",
    "    \n",
    "    # Valor de variable oculta \n",
    "    h_t = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return h_t, C_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar como funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "nh = 2   # Dimensión del vector de variables ocultas\n",
    "nx = 3   # Dimensión del vector de entrada\n",
    "\n",
    "Ui = np.random.standard_normal((nh,nx))\n",
    "Wi = np.random.standard_normal((nh,nh))\n",
    "\n",
    "Uf = np.random.standard_normal((nh,nx))\n",
    "Wf = np.random.standard_normal((nh,nh))\n",
    "\n",
    "Uo = np.random.standard_normal((nh,nx))\n",
    "Wo = np.random.standard_normal((nh,nh))\n",
    "\n",
    "U = np.random.standard_normal((nh,nx))\n",
    "W = np.random.standard_normal((nh,nh))\n",
    "\n",
    "\n",
    "h_t_prev = 2 * np.random.standard_normal((nh,1)) - 1\n",
    "C_t_prev = np.random.standard_normal((nh,1))\n",
    "x_t = 2 * np.random.standard_normal((nx,1)) - 1\n",
    "\n",
    "# Aplicando un solo paso\n",
    "h_t, C_t = forward_LSTM(\n",
    "    [x_t, h_t_prev, C_t_prev],\n",
    "    [Ui, Wi, Uf, Wf, Uo, Wo, U, W] \n",
    ")\n",
    "\n",
    "print(\"\\nValor h_t:\")\n",
    "print(h_t, \"\\n\")\n",
    "\n",
    "print(\"\\nValor C_t:\")\n",
    "print(C_t, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La función `scan`para el cálculo de BPTT\n",
    "\n",
    "La función `scan` se usa para calcular la propagación hacia adelante. Si la funcións e implementa en un *framework* como *Tensorflow* o *pytorch*, entonces se puede ir guardando los gradientes de cada aplicación a lo largo del tiempo y usarlos en el calculo del gradiente para la función de aprendizaje.\n",
    "\n",
    "Aquí solo vamos a tratar de mostrar como funcionaría dicha función, la cual recibe:\n",
    "\n",
    "- `elems` : lista de entradas (`X`)\n",
    "- `weights` : los parámetros que necesita la función de feedforward para su cálculo (pesos)\n",
    "- `h_0` : estado oculto inicial\n",
    "\n",
    "`scan` va por todos los valores de `x` en `elems`, llama la función de feedforward con los argumentos necesarios, guarda el estado oculto `h_t` y agrega el valor de `h_t` a una lista. \n",
    "\n",
    "Vamos a hacer la función de scan para una celda tipo RNN vainilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_V_RNN(elems, weights, h_0=None): # Forward propagation for RNNs\n",
    "    h_t = h_0\n",
    "    h = []\n",
    "    for x in elems:\n",
    "        h_t = forward_V_RNN([x, h_t], weights)\n",
    "        h.append(h_t)\n",
    "    return h, h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar inicializando una posible red RNN vainilla en un probable pornblema de PLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)                 \n",
    "\n",
    "emb = 128                       # Embedding \n",
    "T = 256                         # Tamaño de secuencia de tokens\n",
    "h_dim = 16                      # Estados ocultos\n",
    "\n",
    "h_0 = np.zeros((h_dim, 1))      # Estado inicial\n",
    "\n",
    "# Inicialización aleatoria de pesos y sesgos\n",
    "Whh = np.random.standard_normal((h_dim, h_dim))\n",
    "Wxh = np.random.standard_normal((h_dim, emb))\n",
    "bh = np.random.standard_normal((h_dim, 1))\n",
    "\n",
    "# Inicialización aleatoria de una secuencia de tokens (en embeddings)\n",
    "X = np.random.standard_normal((T, emb, 1))\n",
    "\n",
    "weights = [Whh, Wxh, bh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla RNNs\n",
    "tic = perf_counter()\n",
    "h, h_T = scan_V_RNN(X, weights, h_0)\n",
    "toc = perf_counter()\n",
    "RNN_time=(toc-tic)*1000\n",
    "print (f\"Tomó {RNN_time:.2f}ms ejecutar el método de RNN vainilla.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desarrolla la función de scan para LSTM y prueba con la misma secuencia de entradas para una red LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función scan para LSTM\n",
    "\n",
    "# INICIA CODIGO\n",
    "\n",
    "# TERMINA CODIGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización de variables\n",
    "\n",
    "# INICIA CODIGO\n",
    "\n",
    "# TERMINA CODIGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probando la función de scan\n",
    "\n",
    "# INICIA CODIGO\n",
    "\n",
    "# TERMINA CODIGO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
