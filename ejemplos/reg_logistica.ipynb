{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png)\n",
    "\n",
    "# Ejemplo de Regresión logística\n",
    "\n",
    "## Aprendizaje Automático Aplicado\n",
    "\n",
    "## Maestría en Ciencia de Datos\n",
    "\n",
    "**Julio Waissman**, 2025\n",
    "\n",
    "[Abrir en google Colab](https://colab.research.google.com/github/mcd-unison/aaa-curso/blob/main/ejemplos/reg_logistica.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20,10)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Función logística, función de pérdida y gradiente de la función de costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función logística está dada por \n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}},\n",
    "$$\n",
    "\n",
    "la cual es importante que podamos calcular en forma vectorial. Si bien el calculo es de una sola linea, el uso de estas funciones auxiliares facilitan la legibilidad del código.\n",
    "\n",
    "#### Desarrolla la función logística, la cual se calcule para todos los elementos de un ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistica(z):\n",
    "    \"\"\"\n",
    "    Calcula la función logística para cada elemento de z\n",
    "    \n",
    "    @param z: un ndarray\n",
    "    @return: un ndarray de las mismas dimensiones que z\n",
    "    \"\"\"\n",
    "    # Introduce código aqui (una linea de código)\n",
    "    logistica= None\n",
    "    \n",
    "    return logistica\n",
    "    \n",
    "# prueba que efectivamente funciona la función implementada\n",
    "# si el assert es falso regresa un error de aserción (el testunit de los pobres)\n",
    "assert (np.abs(logistica(np.array([-1, 0, 1])) - np.array([ 0.26894142, 0.5, 0.73105858]))).sum() < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar la función vamos a graficar la función logística en el intervalo [-5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 100)\n",
    "\n",
    "plt.plot( z, logistica(z))\n",
    "plt.title(u'Función logística', fontsize=20)\n",
    "plt.xlabel(r'$z$', fontsize=20)\n",
    "plt.ylabel(r'$\\frac{1}{1 + \\exp(-z)}$', fontsize=26)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez establecida la función logística, vamos a implementar la función de error en linea que se utiliza típicamente en clasificación binaria, la cual está dada por\n",
    "\n",
    "$$\n",
    "E_{in}(w, b) = -\\frac{1}{M} \\sum_{i=1}^M \\left[ y^{(i)}\\log(a^{(i)}) + (1 - y^{(i)})\\log(1 - a^{(i)})\\right],\n",
    "$$\n",
    "\n",
    "donde \n",
    "\n",
    "$$\n",
    "a^{(i)} = \\sigma(z^{(i)}), \\quad z^{(i)} = w^T x^{(i)} + b, \\quad w \\in \\mathrm{R}^n, \\quad b \\in \\mathrm{R}.\n",
    "$$\n",
    "\n",
    "Por supuesto estamos asumiendo que tenemos un conjunto de datos de aprendizaje $\\{(x^{(1)}, y^{(1)}), \\ldots, (x^{(M)}, y^{(M)})\\}$ donde $x^{(i)} \\in \\mathrm{R}^n$ e $y^{(i)} \\in \\{0,1\\}$, las cuales fueron ecuaciones revisadas en clase.\n",
    "\n",
    "#### Implementa la función de error en muestra para un conjunto de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_in(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Calcula E_in para el conjunto dee entrenamiento dado por y y x\n",
    "    y los parametros w y b de la regresión logística\n",
    "    \n",
    "    @param x: un ndarray de dimensión (M, n) con la matriz de diseño\n",
    "    @param y: un ndarray de dimensión (M, ) donde cada entrada es 1.0 o 0.0\n",
    "    @param w: un ndarray de dimensión (n, ) con los pesos\n",
    "    @param b: un flotante con el sesgo\n",
    "\n",
    "    @return: un flotante con el valor de pérdida\n",
    "    \n",
    "    \"\"\" \n",
    "    M = x.shape[0]\n",
    "\n",
    "    # Agrega aqui tu código\n",
    "    z = None\n",
    "    a = None\n",
    "    error_en_muestra = None\n",
    "    \n",
    "    return error_en_muestra\n",
    "    \n",
    "\n",
    "    \n",
    "# Otra vez el testunit del pobre (ya lo calcule yo, pero puedes hacerlo a mano para estar seguro)\n",
    "w = np.array([1])\n",
    "b = 1.0\n",
    "\n",
    "x = np.array([[10],\n",
    "              [-5]])\n",
    "\n",
    "y1 = np.array([1, 0])\n",
    "y2 = np.array([0, 1])\n",
    "y3 = np.array([0, 0])\n",
    "y4 = np.array([1, 1])\n",
    "\n",
    "assert abs(E_in(x, y1, w, b) - 0.01) < 1e-2\n",
    "assert abs(E_in(x, y2, w, b) - 7.5) < 1e-2\n",
    "assert abs(E_in(x, y3, w, b) - 5.5) < 1e-2\n",
    "assert abs(E_in(x, y4, w, b) - 2.0) < 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera, para poder implementar las funciones de aprendizaje, vamos a implementar el gradiente de la función de error. La derivada parcial del error en muestra respecto a cada variable es:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_{in}(w, b)}{\\partial w_j} = -\\frac{1}{M} \\sum_{i=1}^M \\left(y^{(i)} - a^{(i)}\\right)x_j^{(i)} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_{in}(w, b)}{\\partial b} = -\\frac{1}{M} \\sum_{i=1}^M \\left(y^{(i)} - a^{(i)}\\right) \n",
    "$$\n",
    "\n",
    "y a partir de las ecuaciones individuales de puede obtener $\\nabla E_in(\\omega)$, la cual no la vamos a escribir en la libreta para que revisen en sus notas como se puede resolver este problema en forma matricial. \n",
    "\n",
    "#### Implementa (con operaciones matriciales) el calculo del gradiente de la función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente de la función de error, \n",
    "    utilizando una neurona logística, para w y b y conociendo un conjunto de aprendizaje.\n",
    "    \n",
    "    @param x: un ndarray de dimensión (M, n) con la matriz de diseño\n",
    "    @param y: un ndarray de dimensión (M, ) donde cada entrada es 1.0 o 0.0\n",
    "    @param w: un ndarray de dimensión (n, ) con los pesos\n",
    "    @param b: un flotante con el sesgo\n",
    "    \n",
    "    @return: dw, db, un ndarray de mismas dimensiones que w y un flotnte con el cálculo de \n",
    "             la dervada evluada en el punto w y b\n",
    "                 \n",
    "    \"\"\"\n",
    "    M = x.shape[0]\n",
    "\n",
    "    # Agregua aqui tu código\n",
    "    z = None\n",
    "    a = None\n",
    "    \n",
    "    dw = None\n",
    "    db = None\n",
    "    \n",
    "    return dw, db\n",
    "\n",
    "\n",
    "    \n",
    "# Otra vez el testunit del pobre (ya lo calcule yo, pero puedes hacerlo a mano para estar seguro)\n",
    "w = np.array([1])\n",
    "b = 1.0\n",
    "\n",
    "x = np.array([[10],\n",
    "              [-5]])\n",
    "\n",
    "y1 = np.array([1, 0])\n",
    "y2 = np.array([0, 1])\n",
    "y3 = np.array([0, 0])\n",
    "y4 = np.array([1, 1])\n",
    "\n",
    "assert abs(0.00898475 - gradiente(x, y1, w, b)[1]) < 1e-4\n",
    "assert abs(7.45495097 - gradiente(x, y2, w, b)[0]) < 1e-4 \n",
    "assert abs(4.95495097 - gradiente(x, y3, w, b)[0]) < 1e-4 \n",
    "assert abs(-0.49101525 - gradiente(x, y4, w, b)[1]) < 1e-4     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descenso de gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a desarrollar las funciones necesarias para realizar el entrenamiento y encontrar la mejor $\\omega$ de acuero a la función de costos y un conjunto de datos de aprendizaje.\n",
    "\n",
    "Para este problema, vamos a utilizar una base de datos sintética proveniente del curso de [Andrew Ng](www.andrewng.org/) que se encuentra en [Coursera](https://www.coursera.org). Supongamos que pertenecemos al departamente de servicios escolares de la UNISON y vamos a modificar el procedimiento de admisión. En lugar de utilizar un solo exámen (EXCOBA) y la información del cardex de la preparatoria, hemos decidido aplicar dos exámenes, uno sicométrico y otro de habilidades estudiantiles. Dichos exámenes se han aplicado el último año aunque no fueron utilizados como criterio. Así, tenemos un historial entre estudiantes aceptados y resultados de los dos exámenes. El objetivo es hacer un método de regresión que nos permita hacer la admisión a la UNISON tomando en cuenta únicamente los dos exámenes y simplificar el proceso. *Recuerda que esto no es verdad, es solo un ejercicio*.\n",
    "\n",
    "Bien, los datos se encuentran en el archivo `admision.txt` el cual se encuentra en formato `cvs` (osea los valores de las columnas separados por comas. Vamos a leer los datos y graficar la información para entender un poco los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/mcd-unison/aaa-curso/raw/main/ejemplos/admision.txt\"\n",
    "datos = np.loadtxt(url, comments='%', delimiter=',')\n",
    "\n",
    "x, y = datos[:,0:-1], datos[:,-1] \n",
    "\n",
    "plt.plot(x[y == 1, 0], x[y == 1, 1], 'sr', label='aceptados') \n",
    "plt.plot(x[y == 0, 0], x[y == 0, 1], 'ob', label='rechazados')\n",
    "plt.title(u'Ejemplo sintético para regresión logística')\n",
    "plt.xlabel(u'Calificación del primer examen')\n",
    "plt.ylabel(u'Calificación del segundo examen')\n",
    "plt.axis([20, 100, 20, 100])\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vistos los datos un clasificador lineal podría ser una buena solución. Ahora vamos a implementar el método de descenso de gradiente, casi de la misma manera que lo implementamos para regresión lineal (por lotes)\n",
    "\n",
    "#### Implementa el descenso de gradiente para el problema de regresión logística en modo batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descenso_rl_lotes(x, y, nu, max_iter=int(1e4), historial=False):\n",
    "    \"\"\"\n",
    "    Descenso de gradiente por lotes para resolver el problema de regresión logística con un conjunto de aprendizaje\n",
    "\n",
    "    @param x: un ndarray de dimensión (M, n) con la matriz de diseño\n",
    "    @param y: un ndarray de dimensión (M, ) donde cada entrada es 1.0 o 0.0\n",
    "    @param nu: Un flotante (típicamente pequeño) con la tasa de aprendizaje\n",
    "    @param max_iter: Máximo numero de iteraciones. Por default 1e4\n",
    "    @param historial: Un booleano para saber si guardamos el historial del error o no\n",
    "    \n",
    "    @return: w, b, error_hist donde w es ndarray de dimensión (n, ) con los pesos; el flotante b  \n",
    "             con el sesgo y error_hist, un ndarray de dimensión (max_iter,) con el error en muestra \n",
    "             en cada iteración. Si historial == False, entonces error_hist = None.\n",
    "             \n",
    "    \"\"\"\n",
    "    M, n = x.shape\n",
    "    \n",
    "    w = np.zeros(n)\n",
    "    b = 0.0\n",
    "\n",
    "    if historial:\n",
    "        error_hist = np.zeros(max_iter)\n",
    "        error_hist[0] = None\n",
    "    else:\n",
    "        error_hist = None\n",
    "        \n",
    "    for epoch in range(1, max_iter):\n",
    "        # Agregar aqui tu código\n",
    "        #\n",
    "        # Recuerda utilizar las funciones que ya has desarrollado\n",
    "        dw, db = None\n",
    "        \n",
    "        w = None\n",
    "        b = None\n",
    "        \n",
    "        if historial:\n",
    "            error_hist[epoch] = None\n",
    "    return w, b, error_hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar la función de aprendizaje, vamos a aplicarla a nuestro problema de admisión. Primero recuerda que tienes que hacer una exploración para encontrar el mejor valor de $\\epsilon$. Así que utiliza el código de abajo para ajustar $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-6\n",
    "epochs = 50\n",
    "_, _, error_hist = descenso_rl_lotes(x, y, alpha, max_iter=epochs, historial=True)\n",
    "\n",
    "plt.plot(np.arange(mi), error_hist)\n",
    "plt.title(r'Evolucion del valor del error en muestra en las primeras iteraciones con $\\alpha$ = ' + str(alpha))\n",
    "plt.xlabel('iteraciones')\n",
    "plt.ylabel('error dn muestra')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez encontrado el mejor $\\epsilon$, entonces podemos calcular $\\omega$ y $b$ (esto va a tardar bastante), recuerda que el costo final debe de ser lo más cercano a 0 posible, así que agrega cuantas iteraciones sean necesarias (a partir de una función de pérdida con un valor de al rededor de 0.22 ya está bien): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, _ = descenso_rl_lotes(x, y, alpha, max_iter = 1000000)\n",
    "print(\"Los pesos obtenidos son: \\n{}\".format(w))\n",
    "print(\"El sesgo obtenido es: \\n{}\".format(b))\n",
    "print(\"El valor final de la función de pérdida es: {}\".format(E_in(x, y, w, b))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es interesante ver como el descenso de gradiente no es muy eficiente en este tipo de problemas, a pesar de ser problemas de optimización convexos.\n",
    "\n",
    "Bueno, este método nos devuelve $\\omega$ y $b$, pero esto no es suficiente para decir que tenemos un clasificador, ya que un método de clasificación se compone de dos métodos, uno para **aprender** y otro para **predecir**. \n",
    "\n",
    "Recuerda que $a^{(i)} = \\Pr[y^{(i)} = 1 | x^{(i)} ; w, b]$, y a partir de esta probabilidad debemos tomar una desición. Igualmente recuerda que para tomar la desicion no necesitamos calcular el valor de la logística, si conocemos el umbral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desarrolla una función de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(x, w, b, umbral=0.5):\n",
    "    \"\"\"\n",
    "    Predice los valores de y_hat (que solo pueden ser 0 o 1), utilizando el criterio MAP.\n",
    "    \n",
    "    @param x: un ndarray de dimensión (M, n) con la matriz de diseño\n",
    "    @param w: un ndarray de dimensión (n, ) con los pesos\n",
    "    @param b: un flotante con el sesgo\n",
    "    @param umbral: un flotante en [0, 1] con el umbral, por default 0.5 (MAP)\n",
    "\n",
    "    @return: un ndarray de dimensión (M, ) donde cada entrada es 1.0 o 0.0 con la salida estimada\n",
    "    \"\"\"\n",
    "    # Agrega aqui tu código sin utilizar la función logística\n",
    "\n",
    "    umbral_z = None\n",
    "    z = None\n",
    "    y_est = None\n",
    "\n",
    "    return y_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Que tan bueno es este clasificador? ¿Es que implementamos bien el método?\n",
    "\n",
    "Vamos a contestar esto por partes. Primero, vamos a graficar los mismos datos pero vamos a agregar la superficie de separación, la cual en este caso sabemos que es una linea recta. Como sabemos el criterio para decidir si un punto pertenece a la clase distinguida o no es si el valor de $w^T x^{(i)} + b \\ge 0$, por lo que la frontera entre la región donde se escoge una clase de otra se encuentra en:\n",
    "\n",
    "$$\n",
    "0 = b + w_1 x_1  + w_2 x_2,\n",
    "$$\n",
    "\n",
    "y despejando:\n",
    "\n",
    "$$\n",
    "x_2 = -\\frac{b}{w_2} -\\frac{w_1}{w_2}x_1\n",
    "$$\n",
    "\n",
    "son los pares $(x_1, x_2)$ los valores en la forntera. Al ser estos (en este caso) una linea recta solo necesitamos dos para graficar la superficie de separación. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_frontera = np.array([20, 100]) #Los valores mínimo y máximo que tenemos en la gráfica de puntos\n",
    "x2_frontera = -(b / w[1]) - (w[0] / w[1]) * x1_frontera\n",
    "\n",
    "plt.plot(x[y == 1, 0], x[y == 1, 1], 'sr', label='aceptados') \n",
    "plt.plot(x[y == 0, 0], x[y == 0, 1], 'ob', label='rechazados')\n",
    "plt.plot(x1_frontera, x2_frontera, '-m')\n",
    "plt.title(u'Ejemplo sintético para regresión logística')\n",
    "plt.xlabel(u'Calificación del primer examen')\n",
    "plt.ylabel(u'Calificación del segundo examen')\n",
    "plt.axis([20, 100, 20, 100])\n",
    "plt.legend(loc=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para que tengas una idea de lo que debería de salir, anexo una figura obtenida con el código que yo hice \n",
    "\n",
    "![](https://raw.githubusercontent.com/mcd-unison/aaa-curso/main/ejemplos/ejemplo_logistica.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clasificación polinomial\n",
    "\n",
    "Como podemos ver en la gráfica de arriba, parece ser que la regresión logística aceptaría a algunos estudiantes rechazados y rechazaría a algunos que si fueron en realidad aceptados. En todo método de clasificación hay un grado de error, y eso es parte del poder de generalización de los métodos. \n",
    "\n",
    "Sin embargo, una simple inspección visual muestra que, posiblemente, la regresión lineal no es la mejor solución, ya que la frontera entre las dos clases parece ser más bien una curva.\n",
    "\n",
    "¿Que tal si probamos con un clasificador cuadrático? Un clasificador cuadrático no es más que la regresión lineal pero a la que se le agregan todos los atributos que sean una combinación de dos de los atributos. \n",
    "\n",
    "Por ejemplo, si un ejemplo $x = (x_1, x_2, x_3)^T$ se aumenta con todas sus componentes cuadráticas, entonces tenemos los atributos\n",
    "\n",
    "$$\n",
    "\\phi_2(x) = (x_1, x_2, x_3, x_1 x_2, x_1 x_3, x_2 x_3, x_1^2, x_2^2, x_3^2)^T.\n",
    "$$ \n",
    "\n",
    "De la misma manera se pueden obtener clasificadores de orden tres, cuatro, cinco, etc. En general a estos clasificadores se les conoce como **clasificadores polinomiales**. Ahora, para entender bien la idea, vamos a resolver el problema anterior con un clasificador de orden 2. \n",
    "\n",
    "Sin embargo, si luego se quiere hacer el reconocimiento de otros objetos, o cambiar el orden del polinomio, pues se requeriría de reclcular cada vez la expansión polinomial. Vamos a generalizar la obtención de atributos polinomiales con la función `map_poly`, la cual la vamos a desarrollar a continuación.\n",
    "\n",
    "En este caso, la normalización de los datos es muy importante, por lo que se agregan las funciones pertinentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "def map_poly(grad, x):\n",
    "    \"\"\"\n",
    "    Encuentra las características polinomiales hasta el grado grad de la matriz de datos x, \n",
    "    asumiendo que x[:n, 0] es la expansión de orden 1 (los valores de cada atributo)\n",
    "    \n",
    "    @param grad: un entero positivo con el grado de expansión\n",
    "    @param x: un ndarray de dimension (M, n) donde n es el número de atributos\n",
    "    \n",
    "    @return: un ndarray de dimensión (M, n_phi) donde\n",
    "             n_phi = \\sum_{i = 1}^grad fact(i + n - 1)/(fact(i) * fact(n - 1))\n",
    "    \"\"\"\n",
    "    \n",
    "    if int(grad) < 2:\n",
    "        raise ValueError('grad debe de ser mayor a 1')\n",
    "    \n",
    "    M, n = x.shape\n",
    "    atrib = x.copy()\n",
    "    x_phi = x.copy()\n",
    "    for i in range(2, int(grad) + 1):\n",
    "        for comb in combinations_with_replacement(range(n), i):\n",
    "            x_phi = np.c_[x_phi, np.prod(atrib[:, comb], axis=1)]\n",
    "    return x_phi   \n",
    "\n",
    "def medias_std(x):\n",
    "    \"\"\"\n",
    "    Obtiene un vector de medias y desviaciones estandar para normalizar\n",
    "    \n",
    "    @param x: Un ndarray de (M, n) con una matriz de diseño\n",
    "    \n",
    "    @return: mu, des_std dos ndarray de dimensiones (n, ) con las medias y desviaciones estandar\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.mean(x, axis=0), np.std(x, axis=0)\n",
    "\n",
    "def normaliza(x, mu, des_std):\n",
    "    \"\"\"\n",
    "    Normaliza los datos x\n",
    "    \n",
    "    @param x: un ndarray de dimension (M, n) con la matriz de diseño\n",
    "    @param mu: un ndarray (n, ) con las medias\n",
    "    @param des_std: un ndarray (n, ) con las desviaciones estandard\n",
    "    \n",
    "    @return: un ndarray (M, n) con x normalizado\n",
    "    \n",
    "    \"\"\"\n",
    "    return (x - mu) / des_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Realiza la clasificación de los datos utilizando un clasificador cuadrático (recuerda ajustar primero el valor de $\\alpha$)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encuentra phi_x (x son la expansión polinomial de segundo orden, utiliza la función map_poly\n",
    "\n",
    "#--Agrega el código aqui--\n",
    "phi_x =  None\n",
    "mu, de = None\n",
    "phi_x_norm = None\n",
    "\n",
    "# Utiliza la regresión logística\n",
    "alpha = None\n",
    "max_iter = None\n",
    "\n",
    "_, _, hist_loss = descenso_rl_lotes(phi_x_norm, y, alpha, max_iter, historial=True)\n",
    "\n",
    "plt.plot(range(len(hist_loss)), hist_loss)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Evaluación del parámetro alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_norm, b_norm, _ = descenso_rl_lotes(phi_x_norm, y, alpha, 1000)\n",
    "\n",
    "print(\"Los pesos obtenidos son: \\n{}\".format(w_norm))\n",
    "print(\"El sesgo obtenidos es: \\n{}\".format(b_norm))\n",
    "print(\"El valor final de la función de pérdida es: {}\".format(E_in(phi_x_norm, y, w_norm, b_norm))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde se puede encontrar un valor de pérdida de aproximadamente 0.03.\n",
    "\n",
    "Esto lo tenemos que graficar. Pero graficar la separación de datos en una proyección en las primeras dos dimensiones, no es tan sencillo como lo hicimos con una separación lineal, así que vamos a tener que generar un `contour`, y sobre este graficar los datos. Para esto vamos a desarrollar una función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separacion2D(x, y, grado, mu, de, w, b):\n",
    "    \"\"\"\n",
    "    Grafica las primeras dos dimensiones (posiciones 1 y 2) de datos en dos dimensiones \n",
    "    extendidos con un clasificador polinomial así como la separación dada por theta_phi\n",
    "    \n",
    "    \"\"\"\n",
    "    if grado < 2:\n",
    "        raise ValueError('Esta funcion es para graficar separaciones con polinomios mayores a 1')\n",
    "    \n",
    "    x1_min, x1_max = np.min(x[:,0]), np.max(x[:,0])\n",
    "    x2_min, x2_max = np.min(x[:,1]), np.max(x[:,1])\n",
    "    delta1, delta2 = (x1_max - x1_min) * 0.1, (x2_max - x2_min) * 0.1\n",
    "\n",
    "    spanX1 = np.linspace(x1_min - delta1, x1_max + delta1, 600)\n",
    "    spanX2 = np.linspace(x2_min - delta2, x2_max + delta2, 600)\n",
    "    X1, X2 = np.meshgrid(spanX1, spanX2)\n",
    "    \n",
    "    X = normaliza(map_poly(grado, np.c_[X1.ravel(), X2.ravel()]), mu, de)\n",
    "        \n",
    "    Z = predictor(X, w, b)\n",
    "    Z = Z.reshape(X1.shape[0], X1.shape[1])\n",
    "    \n",
    "    # plt.contour(X1, X2, Z, linewidths=0.2, colors='k')\n",
    "    plt.contourf(X1, X2, Z, 1, cmap=plt.cm.binary_r)\n",
    "    plt.plot(x[y > 0.5, 0], x[y > 0.5, 1], 'sr', label='clase positiva')\n",
    "    plt.plot(x[y < 0.5, 0], x[y < 0.5, 1], 'oy', label='clase negativa')\n",
    "    plt.axis([spanX1[0], spanX1[-1], spanX2[0], spanX2[-1]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a probar la función `plot_separacion2D` con los datos de entrenamiento. El comando tarda, ya que estamos haciendo un grid de 200 $\\times$ 200, y realizando evaluaciones individuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_separacion2D(x, y, 2, mu, de, w_norm, b_norm)\n",
    "plt.title(u\"Separación con un clasificador cuadrático\")\n",
    "plt.xlabel(u\"Calificación del primer examen\")\n",
    "plt.ylabel(u\"Calificación del segundo examen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para dar una idea de lo que debería de salir, anexo una figura tal como me salió a mi.\n",
    "\n",
    "![](https://raw.githubusercontent.com/mcd-unison/aaa-curso/main/ejemplos/ejemplo_cuad.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, un clasificador polinomial de orden 2 clasifica mejor los datos de aprendizaje, y además parece suficientemente simple para ser la mejor opción para hacer la predicción. Claro, esto lo sabemos porque pudimos visualizar los datos, y en el fondo estamos haciendo trampa al seleccionar la expansión polinomial a partir de una inspección visual de los datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
