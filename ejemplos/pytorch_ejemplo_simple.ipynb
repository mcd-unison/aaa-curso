{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz8Ny3H6A8TT"
      },
      "source": [
        "![](https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png)\n",
        "\n",
        "# El `Hola mundo`de las redes neuronales con pyTorch\n",
        "\n",
        "## Aprendizaje Automático Aplicado\n",
        "\n",
        "### Maestría en Ciencia de Datos\n",
        "\n",
        "#### **Julio Waissman**, 2024\n",
        "\n",
        "[**Abrir en google Colab**](https://colab.research.google.com/github/mcd-unison/aaa-curso/blob/main/ejemplos/pytorch_ejemplo_simple.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nGmXPhbjA7Rh"
      },
      "outputs": [],
      "source": [
        "# Las librerías necesarias\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as pl\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw4dkNaLE2v8"
      },
      "source": [
        "## Cargando un conjunto de datos\n",
        "\n",
        "El modulo `torchvision.datasets`contiene objetos `Dataset` para muchos conjuntos de datos de entrenamiento bien conocidos de imágenes como CIFAR, COCO etc.. (ver [documentación](https://pytorch.org/vision/stable/datasets.html)).\n",
        "\n",
        "Vamos a usar `transform` para hacer un básico de procesamiento de las imágenes (tambien existe `transform_target`), así cada vez que cargue un conjunto de datos, los irá transformando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RydstKvoCgys"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Descarga el conjunto de datos de entrenamiento\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "# Descarga el conjunto de datos de validación\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFJuit_OF46P"
      },
      "source": [
        "Para poder usarlos en entrenamiento y prueba, hay que pasar los conjuntos de datos en forma de *mini batches*. Para eso usaremos `DataLoader`.\n",
        "\n",
        "Es una función de wrap que permite, a partir de un `Dataset`, generar minibatches, muestrear, revolver aleatoriamente y cargar en diferentes hilos la carga de datos, para usarlos en entrenamiento o predicción."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vYETkbcHF6ae"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8QqwUOjI6ob"
      },
      "source": [
        "Y podemos ver como funciona, sacando el primer batch de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2z3mHsXIyMH"
      },
      "outputs": [],
      "source": [
        "for X, y in test_dataloader:\n",
        "    print(\"Ejemplo de un minibatch a ser usado\")\n",
        "    print(f\"Shape de X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape de y: {y.shape} {y.dtype}\")\n",
        "\n",
        "    pl.figure(figsize=(8,8))\n",
        "    for i in range(64):\n",
        "      pl.subplot(8, 8, i+1)\n",
        "      ax = pl.imshow(np.squeeze(X[i].numpy()), cmap='gray')\n",
        "      pl.grid(visible=False)\n",
        "      pl.axis('off')\n",
        "      pl.title(y[i].numpy(), {'fontsize': 8, 'color': 'red'}, y = 0.91)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzGsatc8KZOr"
      },
      "source": [
        "## Funciones para el aprendizaje\n",
        "\n",
        "Para poder realizar el aprendizaje, necesitamos implementar una función que nos permita hacer un epoch de optimización con los datos de entrenamiento, y otra función que nos permita verificar al final de cada epoch, si estamos aprendiendo o no, usando el conjunto de datos de validación.\n",
        "\n",
        "Estas funciones sob bastante genéricas, aunque hay que programarlas (a diferencia de la función `compile` de TensorFlow):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "b-ICeG1BLyt9"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        y_pred = model(X)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 300 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "yGzF7YUzMjcq"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "            test_loss += loss_fn(y_pred, y).item()\n",
        "            correct += (y_pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Error en conjunto de test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMyLPxXnLqPx"
      },
      "source": [
        "Como puede verse, para hacer un paso de aprendizaje se necesita:\n",
        "\n",
        "1. Un `DataLoader`que genere minibatches\n",
        "2. Un modelo `model` de red neuronal en pytorch. Puede estar preentrenado o ser recien desarrollada su arquitectura.\n",
        "3. Una función de pérdida `loss_fn`\n",
        "4. Un método de optimización.\n",
        "\n",
        "Empecemos por el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrRJUQDdKSfA"
      },
      "source": [
        "## Haciendo un modelo simple\n",
        "\n",
        "Vamos a definir un modelo usando la forma de orientdo a objetos, que al parecer es por mucho la forma preferida en pyTorch, a diferencia de TensorFlow que privilegian la forma funcional.\n",
        "\n",
        "La mayoría de las capas para redes neuronales se encuentran en el módulo [`torch.nn`](https://pytorch.org/docs/stable/nn.html) y las funciones que no requieren parámetros a aprender se encuentran en [`torch.nn.functional`](https://pytorch.org/docs/stable/nn.functional.html#)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1amnK6pEKaC0"
      },
      "outputs": [],
      "source": [
        "# Define el modelo en una combinación de orientado a objetos y funcional\n",
        "class RedDensa(nn.Module):\n",
        "    def __init__(self, hidden_units):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_units, hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_units, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uj5mJa7LlO-"
      },
      "source": [
        "y para poder entrenar se requiere establecer el modelo, asignarlo al dispositivo (CPU o GPU), definir la función de pérdida y el método de optimización:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvfqbA80LrUU"
      },
      "outputs": [],
      "source": [
        "HIDDEN_UNITS = 128\n",
        "EPOCHS = 10\n",
        "\n",
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\" if torch.cuda.is_available() else\n",
        "    \"mps\" if torch.backends.mps.is_available() else\n",
        "    \"cpu\"\n",
        ")\n",
        "print(f\"Usando para el entrenamiento {device}\")\n",
        "\n",
        "modelo_simple = RedDensa(HIDDEN_UNITS).to(device)\n",
        "print(modelo_simple)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(modelo_simple.parameters(), lr=1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9HfR3WINF2T"
      },
      "source": [
        "## Entrenamiento\n",
        "\n",
        "Y el entrenamiento se realiza entonces como:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY_OcbK-NA1R"
      },
      "outputs": [],
      "source": [
        "for t in range(EPOCHS):\n",
        "    print(f\"Epoch {t+1}\\n\" + 36 * '-')\n",
        "\n",
        "    train(train_dataloader, modelo_simple, loss_fn, optimizer)\n",
        "    test(test_dataloader, modelo_simple, loss_fn)\n",
        "\n",
        "print(\"¡Listo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "390l82SGO_iC"
      },
      "source": [
        "## Guardando y cargando modelos\n",
        "\n",
        "Vamos a ver como guardar solo los parámetros (asumiendo que podemos reconstruir un modelo similar) y luego como cargarlo a otro modelo para usarlos en reconocimiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U-TVUl2PEkt"
      },
      "outputs": [],
      "source": [
        "# Guardando solo los parámetros del modelo\n",
        "\n",
        "torch.save(modelo_simple.state_dict(), \"model.pth\")\n",
        "\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUcXhyJ0PVhe"
      },
      "outputs": [],
      "source": [
        "#Cargando solo los parámetros del modelo, cuando conozco el modelo\n",
        "\n",
        "model_loaded = RedDensa(HIDDEN_UNITS).to(device)\n",
        "model_loaded.load_state_dict(torch.load(\"model.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYKLYxyBP3dO"
      },
      "outputs": [],
      "source": [
        "model_loaded.eval()\n",
        "\n",
        "x = torch.stack([test_data[i][0] for i in range(10)])\n",
        "y = test_data.targets[:10]\n",
        "\n",
        "with torch.no_grad():\n",
        "    x = x.to(device)\n",
        "    pred = model_loaded(x)\n",
        "    predicted = pred.argmax(dim=1)\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{y}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpOXWQIhTEd1"
      },
      "source": [
        "## Una red convolucional tipo Le-Net\n",
        "\n",
        "![alt_text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab2/img/convnet_fig.png \"CNN Architecture for MNIST Classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "fxjkHxJj5ehc"
      },
      "outputs": [],
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 24, 3, padding='valid')\n",
        "        self.conv2 = nn.Conv2d(24, 36, 3, padding='valid')\n",
        "\n",
        "        self.fc1 = nn.Linear(900, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        x = self.dropout2(x)\n",
        "        logits = self.fc2(x)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRwlyAIcy9nZ"
      },
      "outputs": [],
      "source": [
        "# Los hiperparámetros\n",
        "LR = 1e-4\n",
        "EPOCHS = 5\n",
        "\n",
        "device = (\n",
        "    \"cuda\" if torch.cuda.is_available() else\n",
        "    \"mps\" if torch.backends.mps.is_available() else\n",
        "    \"cpu\"\n",
        ")\n",
        "print(f\"Usando para el entrenamiento {device}\")\n",
        "\n",
        "# La definición del modelo\n",
        "model_lenet = LeNet().to(device)\n",
        "loss_fn_lenet = nn.CrossEntropyLoss()\n",
        "optimizer_lenet = torch.optim.Adam(model_lenet.parameters(), lr=LR)\n",
        "\n",
        "print(model_lenet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7phx15egPv5"
      },
      "outputs": [],
      "source": [
        "for t in range(EPOCHS):\n",
        "    print(f\"Epoch {t+1}\\n\" + 36 * '-')\n",
        "    train(train_dataloader, model_lenet, loss_fn_lenet, optimizer_lenet)\n",
        "    test(test_dataloader, model_lenet, loss_fn_lenet)\n",
        "print(\"¡Listo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egaHjEEcOH8O"
      },
      "source": [
        "## Para documentar nuestro optimismo\n",
        "\n",
        "¿Podrías replicar el aprendizaje con otras arquitecturas y otros conjuntos de datos?\n",
        "\n",
        "El reto es ahora hacer un modelo (y bajar los datos y usar un `DataLoader`) para dos conjuntos de datos interesantes:\n",
        "\n",
        "1. [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "2. [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)\n",
        "\n",
        "Son dos conjuntos chiquitos y relativamente fáciles, que permiten probar con arquitecturas sencillas de CNNs.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
