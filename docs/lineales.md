---
title: Modelos lineales de aprendizaje 
subtitle: Curso Aprendizaje Automático Aplicado
layout: page
hero_image: https://github.com/mcd-unison/aaa-curso/raw/main/docs/img/intro-banner.jpeg
hero_darken: true
show_sidebar: false
---

## Modelos lineales generativos

1. Naive bayes. [Una presentación de CMU](https://www.cs.cmu.edu/~10601b/slides/NBayes.pdf) y [otra de NYU, MIT-CSAIL](https://people.csail.mit.edu/dsontag/courses/ml12/slides/lecture17.pdf). 


## Modelos lineales generalizados (GLM)

1. [Una presentación medio recargada sobre GLM](https://github.com/mcd-unison/aaa-curso/raw/main/slides/glm.pdf)

2. [Documento sobre GLM](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf) del curso de Andrew Ng de Stanford.

3. La página de wikipedia de [la familia exponencial de distribuciones de probabilidad](https://en.wikipedia.org/wiki/Exponential_family).

4. El modulo [statmodels](https://www.statsmodels.org/stable/index.html) para python, así como un [libro de Peter K. Dunn y Gordon K. Smyth](https://d1wqtxts1xzle7.cloudfront.net/61444643/Generalized_Linear_Models_With_Examples_in_R_-_Dunn20191206-26902-1wyc454.pdf?1575666085=&response-content-disposition=inline%3B+filename%3DSpringer_Texts_in_Statistics_Generalized.pdf&Expires=1644976171&Signature=XAljoYucQiNFVSvkjfXLhJLCK2my2VYBI4zXuekqBNZzgH8hKv1suPVXVBU8pX48nWSQ9fobuh9zfMGd9qnZfo7cINvdpCxiOukHD2RpYodnTDtmQf~~P2oEE1hcX6y2OlPLr4UekzaSeQpVfL4RjelZ3u7Xjn3lMrGEKab5-7s8qw7mIqCXCteT2Rd-Xpyjp7QuYP7VkO6di1eiuL3c4cwdjhQD4u4By0LmHjDOpkUTWsBqA1VY68TtyTBMm8qXdLqr2GqaJTV~XvNWxE~Rxtwd-Z7sx9ll7xES4zD1D4ni52RKWayQJwGoeNG~jF2o0gIfQkWbwv5L1exk9n4pFg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA) sobre GLM en R.


## Maquinas de vectores de soporte

1. [Una presentación bien viejita](https://github.com/mcd-unison/aaa-curso/raw/main/slides/svm_presentacion.pdf) pero que sigue estando decente.

2. Un tutorial muy completo sobre la teoría y los algoritmos de [regresión con máquinas de vectores de soporte](https://alex.smola.org/papers/2004/SmoSch04.pdf).

3. Unas libretas de ejemplo para ejecutar en Kaggle. Un [tutorial de SVM para clasificación](https://www.kaggle.com/prashant111/svm-classifier-tutorial), [otro de múltiples clases, usando búsqueda exhaustiva sobre los valores de los parámetros](https://www.kaggle.com/pranathichunduru/svm-for-multiclass-classification/notebook). Por último [otro tutorial](https://www.kaggle.com/faressayah/support-vector-machine-pca-tutorial-for-beginner/notebook) que usa algunas de las bases de datos más usadas en los tutoriales y cursos de aprendizaje automático.


## Libretas *jupyter*

1. [Una libreta sobre naive Bayes](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.05-Naive-Bayes.ipynb).

2. [Un libreta sobre regresión lineal](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.06-Linear-Regression.ipynb)

3. [Una libreta de máquinas de vectores de soporte](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.07-Support-Vector-Machines.ipynb)






